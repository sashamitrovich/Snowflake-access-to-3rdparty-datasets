{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89a9874d-3579-49aa-bc8e-b726c1e7c20c",
   "metadata": {
    "collapsed": false,
    "name": "intro"
   },
   "source": [
    "# What is this notebook for?\n",
    "\n",
    "Data engineers and data scientists often have a need to import datasets from external sources. \n",
    "\n",
    "With Snowflake's [external network access feature](https://docs.snowflake.com/en/developer-guide/external-network-access/external-network-access-overview) and the [ability to use 3rd party Python packages](https://medium.com/snowflake/snowflake-cli-tutorial-upload-and-use-non-snowflake-anaconda-channel-packages-in-snowflake-888eea8a9742) it is now possible to do that in Snowpark. This notebook is demo purposes only and shouldn't be used as-is in a production environment. Adapt to code to your needs and makes sure you have proper data governance in place (roles and data access)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "snowflake_session"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import json\n",
    "from snowflake.snowpark import Session\n",
    "    \n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark.exceptions import SnowparkSessionException\n",
    "\n",
    "try:\n",
    "    # get the context if this is running within Snowflake (e.g. on a Snowflake Notebook)\n",
    "    session = get_active_session()\n",
    "except SnowparkSessionException:\n",
    "    # otherwise, load creds from a file\n",
    "    snowflake_connection_cfg = json.loads(open('.creds/creds-sample.json').read())\n",
    "\n",
    "# Creating Snowpark Session\n",
    "    session = Session.builder.configs(snowflake_connection_cfg).create()\n",
    "\n",
    "session.custom_package_usage_config = {\"enabled\": True}\n",
    "\n",
    "print('Role:     ', session.get_current_role())\n",
    "print('Warehouse:', session.get_current_warehouse())\n",
    "print('Database: ', session.get_current_database())\n",
    "print('Schema:   ', session.get_current_schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff317cb8-216c-44d1-9033-155ad47bb612",
   "metadata": {
    "collapsed": false,
    "name": "setup_steps"
   },
   "source": [
    "# Setup:\n",
    "- Database\n",
    "- External network access\n",
    "- Logging (event table), just in case things go south"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9d403c-ea7f-43c4-b5f2-b0a8cac82048",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "setup_db"
   },
   "outputs": [],
   "source": [
    "session.use_role('datascientist')\n",
    "session.sql('create schema if not exists snowpark_playground.hug_datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcac4843-bad3-416c-9c72-448dbedc2099",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "setup_external_network_1"
   },
   "outputs": [],
   "source": [
    "sql_text= \"\"\"CREATE OR REPLACE NETWORK RULE huggingface_network_rule\n",
    "  MODE = EGRESS\n",
    "  TYPE = HOST_PORT\n",
    "  VALUE_LIST = ('huggingface.co', 'cdn-lfs-us-1.huggingface.co', 'cdn-lfs.huggingface.co', 's3.amazonaws.com');\"\"\"\n",
    "\n",
    "session.sql(sql_text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d09919d-4716-4921-8688-1a4c3e4c4521",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "setup_external_network_2"
   },
   "outputs": [],
   "source": [
    "sql_text= \"\"\"CREATE OR REPLACE EXTERNAL ACCESS INTEGRATION huggingface_access_integration\n",
    "  ALLOWED_NETWORK_RULES = (huggingface_network_rule)\n",
    "  ENABLED = TRUE;\"\"\"\n",
    "\n",
    "session.sql(sql_text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f41281-7bfe-47bb-b3ea-6b17776dbfab",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "logging"
   },
   "outputs": [],
   "source": [
    "# enable this if logging is required\n",
    "\n",
    "# session.sql('grant all on schema snowpark_playground.hug_datasets to role accountadmin;')\n",
    "# session.use_warehouse('public_xs')\n",
    "# session.use_role('accountadmin')\n",
    "# session.sql('CREATE TABLE IF NOT EXISTS snowpark_playground.hug_datasets.my_events;')\n",
    "# session.sql('ALTER ACCOUNT SET EVENT_TABLE = snowpark_playground.hug_datasets.my_events;')\n",
    "# session.sql('ALTER ACCOUNT SET LOG_LEVEL = DEBUG')\n",
    "# session.sql('grant all on table snowpark_playground.hug_datasets.my_events to role datascientist')\n",
    "# session.use_role('datascientist')\n",
    "\n",
    "session.sql('SHOW PARAMETERS LIKE \\'event_table\\' IN ACCOUNT').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9193f7e2-a457-40a5-b64d-f73dd59eb723",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "use_role"
   },
   "outputs": [],
   "source": [
    "session.use_role('datascientist')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9671785-752b-4b06-9d86-d6b022ddfda2",
   "metadata": {
    "collapsed": false,
    "name": "sp_explanation"
   },
   "source": [
    "# How this works\n",
    "- Patching the datasets library:\n",
    "    - download source bundle: https://files.pythonhosted.org/packages/ff/d5/d0fffd6afdf24c062e3c289f0b13f7636f074005c1e76e322633e8c5508c/datasets-2.18.0.tar.gz\n",
    "    - take out the src/datasets\n",
    "    - patch it:\n",
    "        - remove all usages of os.umask() (**currently not available on Snowpark**)\n",
    "        - removed the import of arrow-hotfix (not available but also not needed for arrow vers. > 14.0)\n",
    "    - zip i.e. create datasets.zip\n",
    "    - upload the .zip to a stage (using the UI, SnowSQL or Snow CLI): snow snowpark package upload -f datasets.zip -s packages -c packages --overwrite\n",
    "- Write the SP\n",
    "    - it has to have an import clause for the package(s) on the stage: IMPORTS = ('@snowpark_playground.custom_packages.packages/datasets.zip')\n",
    "    - \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f10956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import sproc\n",
    "\n",
    "@sproc(packages=['snowflake-snowpark-python', 'pathlib', 'filelock', 'numpy', 'pyarrow', 'dill', 'pandas', 'requests', 'tqdm', 'multiprocess', 'fsspec', 'aiohttp', 'huggingface_hub', 'packaging', 'pyyaml','python-xxhash'],\n",
    "       external_access_integrations=['huggingface_access_integration'], imports=['@snowpark_playground.custom_packages.packages/datasets.zip'],\n",
    "       name=\"get_data\", is_permanent=True, stage_location=\"@snowpark_playground.custom_packages.packages\",\n",
    "       replace=True)\n",
    "def get_data(session: Session, dataset_name: str, table_name:str) -> str:\n",
    "    import datasets\n",
    "    import os\n",
    "    import logging\n",
    "    import shutil\n",
    "\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "    logging.warning(\"Watch out, I'm in the SPROC now!\")\n",
    "\n",
    "    cache_dir=\"/tmp/datasets/.cache\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    os.environ['HOME'] = \"/tmp/datasets\"\n",
    "\n",
    "    dset = datasets.load_dataset(dataset_name, cache_dir=cache_dir)\n",
    "    dset.set_format(\"pandas\")\n",
    "    for dset_chunk in dset['train'].to_pandas(batch_size=10000, batched=True):\n",
    "        # process dataframes\n",
    "        session.write_pandas(dset_chunk, table_name, auto_create_table=True)\n",
    "    shutil.rmtree('/tmp/datasets')\n",
    "    return \"Success!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964f671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sproc_def=\"\"\"\n",
    "CREATE OR REPLACE PROCEDURE snowpark_playground.hug_datasets.huggingface_load(dataset_name STRING, table_name STRING)\n",
    "RETURNS variant\n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION = 3.10\n",
    "HANDLER = 'get_data'\n",
    "EXTERNAL_ACCESS_INTEGRATIONS = (huggingface_access_integration)\n",
    "PACKAGES = ('snowflake-snowpark-python', 'pathlib', 'filelock', 'numpy', 'pyarrow', 'dill', 'pandas', 'requests', 'tqdm', 'multiprocess', 'fsspec', 'aiohttp', 'huggingface_hub', 'packaging', 'pyyaml','python-xxhash')\n",
    "IMPORTS = ('@snowpark_playground.custom_packages.packages/datasets.zip')\n",
    "AS\n",
    "$$\n",
    "import _snowflake\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import pandas\n",
    "import logging\n",
    "import shutil\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logging.warning(\"Watch out, I'm in the UDF now!\")\n",
    "\n",
    "cache_dir=\"/tmp/home/.cache\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "os.environ['HOME'] = \"/tmp/home\"\n",
    "\n",
    "# import datasets as ds\n",
    "\n",
    "# session = requests.Session()\n",
    "def get_data(session, dataset_name, table_name):\n",
    "    import datasets\n",
    "\n",
    "    dset = datasets.load_dataset(dataset_name)\n",
    "    dset.set_format(\"pandas\")\n",
    "    for dset_chunk in dset['train'].to_pandas(batch_size=10000, batched=True):\n",
    "        # process dataframes\n",
    "        session.write_pandas(dset_chunk, table_name, auto_create_table=True)\n",
    "    shutil.rmtree('/tmp/home')\n",
    "    return \"Success!\"\n",
    "$$;\n",
    "\"\"\"\n",
    "\n",
    "session.sql(sproc_def).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d804f7e4-ec03-4f8e-8601-7bc1d0b17490",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "run_sp"
   },
   "outputs": [],
   "source": [
    "db = 'snowpark_playground'\n",
    "stage = 'hug_datasets'\n",
    "\n",
    "dataset_name='ai4privacy/pii-masking-300k'\n",
    "table_name = 'ai4privacy_pii_masking_300k'.upper()\n",
    "\n",
    "session.use_database(db)\n",
    "session.use_schema(stage)\n",
    "\n",
    "session.use_warehouse('ds_m_snowpark')\n",
    "session.sql(f\"drop table if exists {table_name}\").collect()\n",
    "session.call(f\"{db}.{stage}.huggingface_load\",dataset_name, table_name)\n",
    "\n",
    "session.use_warehouse('ds_xs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa6209e-c17c-4687-9f95-bcaba0af6919",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "show_results"
   },
   "outputs": [],
   "source": [
    "ds = session.table(f\"{db}.{stage}.{table_name}\")\n",
    "ds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fc72cb-bd8e-4bfa-a690-c9ec3aef3c05",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "golden",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
